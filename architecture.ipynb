{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Deep Variational Reinforcement Learning Architecture\n",
    "\n",
    "This notebook provides a detailed visualization of the MA-DVRL architecture, including network components, mathematical foundations, and training process.\n",
    "\n",
    "## Setup\n",
    "First, let's import the required packages for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Architecture\n",
    "\n",
    "The MA-DVRL model consists of several key components:\n",
    "1. Encoder Network (for private beliefs)\n",
    "2. Inference Network (for opponent modeling)\n",
    "3. Policy Network (for action selection)\n",
    "4. Value Network (for state-value estimation)\n",
    "\n",
    "Let's visualize each component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_architecture_diagram():\n",
    "    dot = graphviz.Digraph(comment='MA-DVRL Architecture')\n",
    "    dot.attr(rankdir='LR')\n",
    "    \n",
    "    # Add nodes\n",
    "    with dot.subgraph(name='cluster_0') as c:\n",
    "        c.attr(label='Encoder Network')\n",
    "        c.node('obs', 'Observation\\n(cards, pot, etc.)')\n",
    "        c.node('enc1', 'FC Layer\\n(256)')\n",
    "        c.node('enc2', 'FC Layer\\n(256)')\n",
    "        c.node('belief', 'Belief State\\n(256)')\n",
    "    \n",
    "    with dot.subgraph(name='cluster_1') as c:\n",
    "        c.attr(label='Inference Network')\n",
    "        c.node('opp_obs', 'Opponent Actions\\n+ Public Info')\n",
    "        c.node('inf1', 'FC Layer\\n(256)')\n",
    "        c.node('inf2', 'FC Layer\\n(256)')\n",
    "        c.node('opp_belief', 'Opponent Belief\\n(256)')\n",
    "    \n",
    "    with dot.subgraph(name='cluster_2') as c:\n",
    "        c.attr(label='Policy Network')\n",
    "        c.node('joint', 'Joint Belief\\n(512)')\n",
    "        c.node('pol1', 'FC Layer\\n(256)')\n",
    "        c.node('pol2', 'FC Layer\\n(256)')\n",
    "        c.node('action', 'Action Probs\\n(4)')\n",
    "    \n",
    "    with dot.subgraph(name='cluster_3') as c:\n",
    "        c.attr(label='Value Network')\n",
    "        c.node('val1', 'FC Layer\\n(256)')\n",
    "        c.node('val2', 'FC Layer\\n(256)')\n",
    "        c.node('value', 'State Value\\n(1)')\n",
    "    \n",
    "    # Add edges\n",
    "    dot.edge('obs', 'enc1')\n",
    "    dot.edge('enc1', 'enc2')\n",
    "    dot.edge('enc2', 'belief')\n",
    "    \n",
    "    dot.edge('opp_obs', 'inf1')\n",
    "    dot.edge('inf1', 'inf2')\n",
    "    dot.edge('inf2', 'opp_belief')\n",
    "    \n",
    "    dot.edge('belief', 'joint')\n",
    "    dot.edge('opp_belief', 'joint')\n",
    "    dot.edge('joint', 'pol1')\n",
    "    dot.edge('pol1', 'pol2')\n",
    "    dot.edge('pol2', 'action')\n",
    "    \n",
    "    dot.edge('joint', 'val1')\n",
    "    dot.edge('val1', 'val2')\n",
    "    dot.edge('val2', 'value')\n",
    "    \n",
    "    return dot\n",
    "\n",
    "architecture = create_architecture_diagram()\n",
    "display(architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Foundations\n",
    "\n",
    "### 1. Variational Inference\n",
    "\n",
    "The MA-DVRL model uses variational inference to learn belief states. The key equations are:\n",
    "\n",
    "**Prior Distribution (Encoder)**:\n",
    "$$p(z_t|o_t) = \\mathcal{N}(\\mu_\\phi(o_t), \\sigma_\\phi(o_t))$$\n",
    "\n",
    "where:\n",
    "- $z_t$ is the belief state at time t\n",
    "- $o_t$ is the observation at time t\n",
    "- $\\phi$ are the encoder network parameters\n",
    "\n",
    "**Posterior Distribution (Inference)**:\n",
    "$$q(z_t|o_{1:t}, a_{1:t-1}) = \\mathcal{N}(\\mu_\\theta(h_t), \\sigma_\\theta(h_t))$$\n",
    "\n",
    "where:\n",
    "- $h_t$ is the history encoding up to time t\n",
    "- $\\theta$ are the inference network parameters\n",
    "\n",
    "**ELBO Loss**:\n",
    "$$\\mathcal{L}_{ELBO} = \\mathbb{E}_{q(z)}[\\log p(o|z)] - D_{KL}(q(z|o,a)||p(z|o))$$\n",
    "\n",
    "### 2. Policy Learning\n",
    "\n",
    "The policy is learned using the following objective:\n",
    "\n",
    "$$\\mathcal{L}_{policy} = \\mathbb{E}_{\\pi_\\psi}[\\sum_{t=0}^T r_t] + \\alpha H(\\pi_\\psi)$$\n",
    "\n",
    "where:\n",
    "- $\\pi_\\psi$ is the policy network with parameters $\\psi$\n",
    "- $r_t$ is the reward at time t\n",
    "- $H(\\pi_\\psi)$ is the entropy of the policy\n",
    "- $\\alpha$ is the entropy coefficient\n",
    "\n",
    "### 3. Value Function\n",
    "\n",
    "The value function is learned using TD learning:\n",
    "\n",
    "$$\\mathcal{L}_{value} = \\mathbb{E}[(V_\\omega(s_t) - (r_t + \\gamma V_\\omega(s_{t+1})))^2]$$\n",
    "\n",
    "where:\n",
    "- $V_\\omega$ is the value network with parameters $\\omega$\n",
    "- $\\gamma$ is the discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_training_flow_diagram():\n",
    "    dot = graphviz.Digraph(comment='MA-DVRL Training Flow')\n",
    "    dot.attr(rankdir='TB')\n",
    "    \n",
    "    # Add nodes\n",
    "    dot.node('obs', 'Observation')\n",
    "    dot.node('enc', 'Encoder\\nNetwork')\n",
    "    dot.node('inf', 'Inference\\nNetwork')\n",
    "    dot.node('belief', 'Belief\\nState')\n",
    "    dot.node('policy', 'Policy\\nNetwork')\n",
    "    dot.node('value', 'Value\\nNetwork')\n",
    "    dot.node('action', 'Action')\n",
    "    dot.node('env', 'Environment')\n",
    "    dot.node('reward', 'Reward')\n",
    "    \n",
    "    # Add edges with labels\n",
    "    dot.edge('obs', 'enc', 'o_t')\n",
    "    dot.edge('obs', 'inf', 'o_{1:t}')\n",
    "    dot.edge('enc', 'belief', 'p(z_t|o_t)')\n",
    "    dot.edge('inf', 'belief', 'q(z_t|o,a)')\n",
    "    dot.edge('belief', 'policy', 'z_t')\n",
    "    dot.edge('belief', 'value', 'z_t')\n",
    "    dot.edge('policy', 'action', '\\pi(a_t|z_t)')\n",
    "    dot.edge('action', 'env')\n",
    "    dot.edge('env', 'reward')\n",
    "    dot.edge('env', 'obs', 'o_{t+1}')\n",
    "    \n",
    "    return dot\n",
    "\n",
    "training_flow = create_training_flow_diagram()\n",
    "display(training_flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Details\n",
    "\n",
    "### Encoder Network Architecture\n",
    "```\n",
    "Input (obs_dim) → FC(256) → ReLU → FC(256) → ReLU → \n",
    "                                              → FC(256) → μ\n",
    "                                              → FC(256) → σ\n",
    "```\n",
    "\n",
    "### Inference Network Architecture\n",
    "```\n",
    "Input (hist_dim) → FC(256) → ReLU → FC(256) → ReLU → \n",
    "                                              → FC(256) → μ\n",
    "                                              → FC(256) → σ\n",
    "```\n",
    "\n",
    "### Policy Network Architecture\n",
    "```\n",
    "Input (512) → FC(256) → ReLU → FC(256) → ReLU → FC(4) → Softmax\n",
    "```\n",
    "\n",
    "### Value Network Architecture\n",
    "```\n",
    "Input (512) → FC(256) → ReLU → FC(256) → ReLU → FC(1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process\n",
    "\n",
    "1. **Forward Pass**:\n",
    "   - Encode observation to get prior belief\n",
    "   - Use inference network to get posterior belief\n",
    "   - Sample belief state from posterior\n",
    "   - Get action probabilities and value estimate\n",
    "\n",
    "2. **Action Selection**:\n",
    "   - Sample action from policy\n",
    "   - Execute in environment\n",
    "   - Store transition (o, a, r, o')\n",
    "\n",
    "3. **Backward Pass**:\n",
    "   - Compute ELBO loss\n",
    "   - Compute policy gradient loss\n",
    "   - Compute value loss\n",
    "   - Update all networks\n",
    "\n",
    "The total loss is:\n",
    "\n",
    "$$\\mathcal{L}_{total} = \\mathcal{L}_{ELBO} + \\lambda_1\\mathcal{L}_{policy} + \\lambda_2\\mathcal{L}_{value}$$\n",
    "\n",
    "where $\\lambda_1$ and $\\lambda_2$ are weighting coefficients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
