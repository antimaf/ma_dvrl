{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Deep Variational Reinforcement Learning for Poker\n",
    "\n",
    "This notebook implements and trains a MA-DVRL model for playing heads-up Texas Hold'em Poker. The model combines variational inference with deep reinforcement learning to handle partial observability and opponent modeling in a competitive setting.\n",
    "\n",
    "## Setup\n",
    "First, let's import the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from environments.poker_env import PokerEnv\n",
    "from models.poker_dvrl import PokerMADVRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Set up training parameters and initialize Weights & Biases for experiment tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    'num_episodes': 10000,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 3e-4,\n",
    "    'gamma': 0.99,\n",
    "    'card_dim': 32,\n",
    "    'belief_dim': 256,\n",
    "    'hidden_dim': 128,\n",
    "    'num_heads': 4,\n",
    "    'initial_chips': 1000,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "# Initialize wandb\n",
    "use_wandb = True  # Set to False to disable W&B logging\n",
    "if use_wandb:\n",
    "    wandb.init(\n",
    "        project='poker-dvrl',\n",
    "        config=config,\n",
    "        name=f'poker_dvrl_{datetime.now():%Y%m%d_%H%M%S}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "Define utility functions for training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def convert_to_tensor(obs_dict, device):\n",
    "    \"\"\"Convert numpy observations to PyTorch tensors.\"\"\"\n",
    "    return {\n",
    "        i: {\n",
    "            k: torch.as_tensor(v, device=device)\n",
    "            for k, v in obs.items()\n",
    "        }\n",
    "        for i, obs in obs_dict.items()\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, env, config, num_episodes=100):\n",
    "    \"\"\"Evaluate model performance.\"\"\"\n",
    "    model.eval()\n",
    "    rewards = []\n",
    "    \n",
    "    for _ in tqdm(range(num_episodes), desc='Evaluating'):\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Model action for player 0\n",
    "            obs_tensor = convert_to_tensor(obs, config['device'])\n",
    "            with torch.no_grad():\n",
    "                actions_prob, _ = model(obs_tensor)\n",
    "                action_0 = torch.argmax(actions_prob[0]).item()\n",
    "            \n",
    "            # Random action for player 1\n",
    "            valid_actions = obs[1]['valid_actions']\n",
    "            valid_indices = np.where(valid_actions == 1)[0]\n",
    "            action_1 = np.random.choice(valid_indices)\n",
    "            \n",
    "            # Take actions\n",
    "            obs, rewards, dones, _ = env.step({0: action_0, 1: action_1})\n",
    "            episode_reward += rewards[0]  # Track rewards for player 0\n",
    "            done = any(dones.values())\n",
    "        \n",
    "        rewards.append(episode_reward)\n",
    "    \n",
    "    model.train()\n",
    "    return np.mean(rewards), np.std(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Environment and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize environment and model\n",
    "env = PokerEnv(initial_chips=config['initial_chips'])\n",
    "model = PokerMADVRL(\n",
    "    card_dim=config['card_dim'],\n",
    "    belief_dim=config['belief_dim'],\n",
    "    hidden_dim=config['hidden_dim'],\n",
    "    num_heads=config['num_heads']\n",
    ").to(config['device'])\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Train the model with logging and visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training metrics\n",
    "episode_rewards = deque(maxlen=100)\n",
    "best_reward = float('-inf')\n",
    "\n",
    "# Training loop\n",
    "progress_bar = tqdm(range(config['num_episodes']), desc='Training')\n",
    "for episode in progress_bar:\n",
    "    obs = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_loss = 0\n",
    "    num_steps = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Convert observations to tensors\n",
    "        obs_tensor = convert_to_tensor(obs, config['device'])\n",
    "        \n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            actions_prob, opponent_preds = model(obs_tensor)\n",
    "            \n",
    "            # Sample actions from the policy\n",
    "            actions = {\n",
    "                i: torch.multinomial(probs, 1).item()\n",
    "                for i, probs in actions_prob.items()\n",
    "            }\n",
    "        \n",
    "        # Take actions in the environment\n",
    "        next_obs, rewards, dones, _ = env.step(actions)\n",
    "        done = any(dones.values())\n",
    "        \n",
    "        # Convert everything to tensors for training\n",
    "        next_obs_tensor = convert_to_tensor(next_obs, config['device'])\n",
    "        actions_tensor = {\n",
    "            i: F.one_hot(torch.tensor([a], device=config['device']), 4).float()\n",
    "            for i, a in actions.items()\n",
    "        }\n",
    "        rewards_tensor = {\n",
    "            i: torch.tensor([r], device=config['device']).float()\n",
    "            for i, r in rewards.items()\n",
    "        }\n",
    "        dones_tensor = {\n",
    "            i: torch.tensor([d], device=config['device']).float()\n",
    "            for i, d in dones.items()\n",
    "        }\n",
    "        \n",
    "        # Compute loss and update model\n",
    "        loss = model.get_loss(\n",
    "            obs_tensor,\n",
    "            actions_tensor,\n",
    "            rewards_tensor,\n",
    "            next_obs_tensor,\n",
    "            dones_tensor,\n",
    "            config['gamma']\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        episode_reward += sum(rewards.values())\n",
    "        episode_loss += loss.item()\n",
    "        num_steps += 1\n",
    "        obs = next_obs\n",
    "    \n",
    "    # Log episode metrics\n",
    "    episode_rewards.append(episode_reward)\n",
    "    avg_reward = sum(episode_rewards) / len(episode_rewards)\n",
    "    avg_loss = episode_loss / num_steps\n",
    "    \n",
    "    # Update progress bar\n",
    "    progress_bar.set_postfix({\n",
    "        'avg_reward': f'{avg_reward:.2f}',\n",
    "        'loss': f'{avg_loss:.4f}'\n",
    "    })\n",
    "    \n",
    "    # Log to wandb if enabled\n",
    "    if use_wandb:\n",
    "        wandb.log({\n",
    "            'episode': episode,\n",
    "            'reward': episode_reward,\n",
    "            'avg_reward': avg_reward,\n",
    "            'loss': avg_loss,\n",
    "            'steps': num_steps\n",
    "        })\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_reward > best_reward:\n",
    "        best_reward = avg_reward\n",
    "        torch.save(\n",
    "            {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'episode': episode,\n",
    "                'best_reward': best_reward,\n",
    "                'config': config\n",
    "            },\n",
    "            'poker_dvrl_best.pt'\n",
    "        )\n",
    "        if use_wandb:\n",
    "            wandb.save('poker_dvrl_best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "Test the trained model against a random opponent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate the best model\n",
    "checkpoint = torch.load('poker_dvrl_best.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "mean_reward, std_reward = evaluate_model(model, env, config)\n",
    "\n",
    "print(f'Evaluation Results:')\n",
    "print(f'Mean Reward: {mean_reward:.2f} Â± {std_reward:.2f}')\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
