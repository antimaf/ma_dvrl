{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poker Agent Model Comparison\n",
    "\n",
    "This notebook compares different approaches to playing heads-up Texas Hold'em Poker:\n",
    "- MA-DVRL (Multi-Agent Deep Variational Reinforcement Learning)\n",
    "- CFR+ (Counterfactual Regret Minimization Plus)\n",
    "- PPO (Proximal Policy Optimization)\n",
    "- Policy Gradient\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from environments.poker_env import PokerEnv\n",
    "from models.poker_dvrl import PokerMADVRL\n",
    "from models.cfr_plus import CFRPlusTrainer\n",
    "from models.ppo import PPOTrainer\n",
    "from models.policy_gradient import PolicyGradientTrainer\n",
    "from utils.plotting import (\n",
    "    plot_training_curves,\n",
    "    plot_model_comparison,\n",
    "    plot_win_rates,\n",
    "    plot_learning_curves_comparison\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    'num_episodes': 10000,\n",
    "    'eval_episodes': 1000,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 3e-4,\n",
    "    'gamma': 0.99,\n",
    "    'card_dim': 32,\n",
    "    'belief_dim': 256,\n",
    "    'hidden_dim': 128,\n",
    "    'num_heads': 4,\n",
    "    'initial_chips': 1000,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    project='poker-comparison',\n",
    "    config=config,\n",
    "    name=f'model_comparison_{datetime.now():%Y%m%d_%H%M%S}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_dvrl(config):\n",
    "    \"\"\"Train MA-DVRL model.\"\"\"\n",
    "    env = PokerEnv(initial_chips=config['initial_chips'])\n",
    "    model = PokerMADVRL(\n",
    "        card_dim=config['card_dim'],\n",
    "        belief_dim=config['belief_dim'],\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        num_heads=config['num_heads']\n",
    "    ).to(config['device'])\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    history = {'reward': [], 'loss': []}\n",
    "    \n",
    "    for episode in tqdm(range(config['num_episodes']), desc='Training MA-DVRL'):\n",
    "        episode_reward, metrics = train_dvrl_episode(env, model, optimizer, config)\n",
    "        history['reward'].append(episode_reward)\n",
    "        history['loss'].append(metrics['loss'])\n",
    "        \n",
    "    return model, history\n",
    "\n",
    "def train_cfr(config):\n",
    "    \"\"\"Train CFR+ model.\"\"\"\n",
    "    env = PokerEnv(initial_chips=config['initial_chips'])\n",
    "    trainer = CFRPlusTrainer()\n",
    "    history = {'reward': [], 'regret_sum': []}\n",
    "    \n",
    "    for episode in tqdm(range(config['num_episodes']), desc='Training CFR+'):\n",
    "        reward, metrics = trainer.train_iteration(env)\n",
    "        history['reward'].append(reward)\n",
    "        history['regret_sum'].append(metrics['regret_sum'])\n",
    "        \n",
    "    return trainer, history\n",
    "\n",
    "def train_ppo(config):\n",
    "    \"\"\"Train PPO model.\"\"\"\n",
    "    env = PokerEnv(initial_chips=config['initial_chips'])\n",
    "    trainer = PPOTrainer(\n",
    "        input_dim=env.observation_space.shape[0],\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        device=config['device']\n",
    "    )\n",
    "    history = {'reward': [], 'policy_loss': [], 'value_loss': []}\n",
    "    \n",
    "    for episode in tqdm(range(config['num_episodes']), desc='Training PPO'):\n",
    "        episode_reward, metrics = train_ppo_episode(env, trainer, config)\n",
    "        history['reward'].append(episode_reward)\n",
    "        history['policy_loss'].append(metrics['policy_loss'])\n",
    "        history['value_loss'].append(metrics['value_loss'])\n",
    "        \n",
    "    return trainer, history\n",
    "\n",
    "def train_pg(config):\n",
    "    \"\"\"Train Policy Gradient model.\"\"\"\n",
    "    env = PokerEnv(initial_chips=config['initial_chips'])\n",
    "    trainer = PolicyGradientTrainer(\n",
    "        input_dim=env.observation_space.shape[0],\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        device=config['device']\n",
    "    )\n",
    "    history = {'reward': [], 'policy_loss': []}\n",
    "    \n",
    "    for episode in tqdm(range(config['num_episodes']), desc='Training PG'):\n",
    "        episode_reward, metrics = train_pg_episode(env, trainer, config)\n",
    "        history['reward'].append(episode_reward)\n",
    "        history['policy_loss'].append(metrics['policy_loss'])\n",
    "        \n",
    "    return trainer, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_model(model, env, num_episodes=1000):\n",
    "    \"\"\"Evaluate model against random opponent.\"\"\"\n",
    "    rewards = []\n",
    "    wins = 0\n",
    "    \n",
    "    for _ in tqdm(range(num_episodes), desc='Evaluating'):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Model plays as player 0\n",
    "            if env.current_player == 0:\n",
    "                action = model.get_action(obs)\n",
    "            else:\n",
    "                # Random opponent\n",
    "                valid_actions = obs[1]['valid_actions']\n",
    "                valid_indices = np.where(valid_actions == 1)[0]\n",
    "                action = np.random.choice(valid_indices)\n",
    "            \n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward[0]\n",
    "            \n",
    "        rewards.append(episode_reward)\n",
    "        wins += episode_reward > 0\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': np.mean(rewards),\n",
    "        'std_reward': np.std(rewards),\n",
    "        'win_rate': wins / num_episodes\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train all models\n",
    "models = {}\n",
    "histories = {}\n",
    "\n",
    "print('Training MA-DVRL...')\n",
    "models['MA-DVRL'], histories['MA-DVRL'] = train_dvrl(config)\n",
    "\n",
    "print('\\nTraining CFR+...')\n",
    "models['CFR+'], histories['CFR+'] = train_cfr(config)\n",
    "\n",
    "print('\\nTraining PPO...')\n",
    "models['PPO'], histories['PPO'] = train_ppo(config)\n",
    "\n",
    "print('\\nTraining Policy Gradient...')\n",
    "models['PG'], histories['PG'] = train_pg(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate all models\n",
    "env = PokerEnv(initial_chips=config['initial_chips'])\n",
    "results = {}\n",
    "win_rates = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'\\nEvaluating {name}...')\n",
    "    eval_metrics = evaluate_model(model, env, config['eval_episodes'])\n",
    "    results[name] = eval_metrics\n",
    "    win_rates[name] = eval_metrics['win_rate']\n",
    "\n",
    "# Plot learning curves\n",
    "fig = plot_learning_curves_comparison(\n",
    "    histories,\n",
    "    metric='reward',\n",
    "    title='Learning Curves Comparison'\n",
    ")\n",
    "wandb.log({'learning_curves': wandb.Image(fig)})\n",
    "plt.show()\n",
    "\n",
    "# Plot model comparison\n",
    "fig = plot_model_comparison(\n",
    "    results,\n",
    "    metrics=['mean_reward', 'win_rate'],\n",
    "    title='Model Performance Comparison'\n",
    ")\n",
    "wandb.log({'model_comparison': wandb.Image(fig)})\n",
    "plt.show()\n",
    "\n",
    "# Plot win rates\n",
    "fig = plot_win_rates(win_rates)\n",
    "wandb.log({'win_rates': wandb.Image(fig)})\n",
    "plt.show()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Let's analyze the results of our comparison:\n",
    "\n",
    "1. **Learning Speed**\n",
    "   - Which algorithm learned fastest?\n",
    "   - Were there any significant plateaus in learning?\n",
    "\n",
    "2. **Final Performance**\n",
    "   - Which algorithm achieved the highest mean reward?\n",
    "   - Which algorithm had the most consistent performance (lowest std)?\n",
    "   - How do the win rates compare?\n",
    "\n",
    "3. **Strengths and Weaknesses**\n",
    "   - MA-DVRL: How well did it handle partial observability?\n",
    "   - CFR+: Did it find Nash equilibrium strategies?\n",
    "   - PPO: How stable was the learning process?\n",
    "   - Policy Gradient: Did it suffer from high variance?\n",
    "\n",
    "4. **Practical Considerations**\n",
    "   - Training time and computational requirements\n",
    "   - Ease of implementation and tuning\n",
    "   - Scalability to larger game variants"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
